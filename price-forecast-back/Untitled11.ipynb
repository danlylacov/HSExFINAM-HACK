{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c577ef6-0d0e-4823-8cb6-4d8162d3f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('–∏—Ç–æ–≥2.csv')\n",
    "\n",
    "def json_to_dataframe_simple(json_file):\n",
    "    df = pd.read_json(json_file)\n",
    "    return df\n",
    "\n",
    "data2 = json_to_dataframe_simple('response.json')\n",
    "data2=data2[:50]\n",
    "result = pd.concat([data, data2],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4563036-33b0-4547-a5bd-7a53e0b5e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24136978-9d37-406b-88c0-262794a812a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§–£–ù–ö–¶–ò–ò –î–õ–Ø –û–¶–ï–ù–ö–ò –ú–ï–¢–†–ò–ö\n",
    "def calculate_competition_metrics(y_true, y_pred, probabilities=None, base_mae=None, base_brier=None):\n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. MAE (Mean Absolute Error)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    metrics['MAE'] = mae\n",
    "    \n",
    "    # –ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π MAE (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–µ–π–∑–ª–∞–π–Ω–∞)\n",
    "    if base_mae is not None and base_mae > 0:\n",
    "        mae_norm = max(0, 1 - (mae / base_mae))\n",
    "        metrics['MAE_norm'] = mae_norm\n",
    "    else:\n",
    "        metrics['MAE_norm'] = 0.0\n",
    "    \n",
    "    # 2. Brier Score (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏)\n",
    "    if probabilities is not None:\n",
    "        # –°–æ–∑–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏: 1 –µ—Å–ª–∏ —Ä–æ—Å—Ç, 0 –µ—Å–ª–∏ –ø–∞–¥–µ–Ω–∏–µ\n",
    "        true_directions = (y_true > 0).astype(int)\n",
    "        brier = np.mean((true_directions - probabilities) ** 2)\n",
    "        metrics['Brier'] = brier\n",
    "        \n",
    "        # –ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Brier\n",
    "        if base_brier is not None and base_brier > 0:\n",
    "            brier_norm = max(0, 1 - (brier / base_brier))\n",
    "            metrics['Brier_norm'] = brier_norm\n",
    "        else:\n",
    "            metrics['Brier_norm'] = 0.0\n",
    "    else:\n",
    "        metrics['Brier'] = None\n",
    "        metrics['Brier_norm'] = 0.0\n",
    "    \n",
    "    # 3. Directional Accuracy (DA)\n",
    "    true_sign = np.sign(y_true)\n",
    "    pred_sign = np.sign(y_pred)\n",
    "    da = np.mean(true_sign == pred_sign)\n",
    "    metrics['DA'] = da\n",
    "    \n",
    "    # 4. –ò—Ç–æ–≥–æ–≤—ã–π Score\n",
    "    score_components = []\n",
    "    score_components.append(0.7 * metrics['MAE_norm'])\n",
    "    score_components.append(0.3 * metrics['Brier_norm'])\n",
    "    score_components.append(0.1 * metrics['DA'])\n",
    "    \n",
    "    metrics['Final_Score'] = sum(score_components)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_returns(prices, horizon=1):\n",
    "    \"\"\"\n",
    "    –†–∞—Å—á–µ—Ç –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –Ω–∞ –∑–∞–¥–∞–Ω–Ω–æ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–µ\n",
    "    \"\"\"\n",
    "    future_prices = prices.shift(-horizon)\n",
    "    returns = (future_prices / prices) - 1\n",
    "    return returns\n",
    "\n",
    "def evaluate_all_targets(test_df, predictions):\n",
    "    print(\"\\n=== –û–¶–ï–ù–ö–ê –ü–û –í–°–ï–ú –¶–ï–õ–ï–í–´–ú –ü–ï–†–ï–ú–ï–ù–ù–´–ú ===\")\n",
    "    \n",
    "    results = {}\n",
    "    for target in ['open', 'high', 'low', 'close', 'volume']:\n",
    "        if target in predictions and target in test_df.columns:\n",
    "            y_true = test_df[target].values\n",
    "            y_pred = predictions[target]\n",
    "            \n",
    "            # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã\n",
    "            min_len = min(len(y_true), len(y_pred))\n",
    "            y_true = y_true[:min_len]\n",
    "            y_pred = y_pred[:min_len]\n",
    "            \n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "            \n",
    "            # MAPE —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                mape = np.mean(np.abs((y_true - y_pred) / np.where(y_true != 0, y_true, 1))) * 100\n",
    "            \n",
    "            results[target] = {\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'MAPE': mape\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{target.upper()}:\")\n",
    "            print(f\"  MAE: {mae:.4f}\")\n",
    "            print(f\"  RMSE: {rmse:.4f}\")\n",
    "            print(f\"  MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_ensemble_with_metrics(test_df, predictions, horizon=1):\n",
    "    print(f\"\\n=== –û–¶–ï–ù–ö–ê –ê–ù–°–ê–ú–ë–õ–Ø –ù–ê –ì–û–†–ò–ó–û–ù–¢–ï {horizon} –î–ù–ï–ô ===\")\n",
    "    \n",
    "    # –†–∞—Å—á–µ—Ç –∏—Å—Ç–∏–Ω–Ω–æ–π –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    true_prices = test_df['close'].values\n",
    "    true_returns = calculate_returns(pd.Series(true_prices), horizon=horizon)\n",
    "    \n",
    "    # –£–¥–∞–ª—è–µ–º NaN –≤ –Ω–∞—á–∞–ª–µ (–∏–∑-–∑–∞ shift)\n",
    "    valid_indices = ~np.isnan(true_returns)\n",
    "    true_returns = true_returns[valid_indices]\n",
    "    \n",
    "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ —Ü–µ–Ω—ã –∑–∞–∫—Ä—ã—Ç–∏—è\n",
    "    predicted_prices = predictions['close']\n",
    "    \n",
    "    # –û–±—Ä–µ–∑–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ –¥–ª–∏–Ω–µ true_returns\n",
    "    predicted_prices = predicted_prices[:len(true_returns)]\n",
    "    \n",
    "    # –†–∞—Å—á–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–π –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏\n",
    "    current_prices = true_prices[:len(predicted_prices)]\n",
    "    predicted_returns = (predicted_prices / current_prices) - 1\n",
    "    \n",
    "    print(f\"–î–∏–∞–ø–∞–∑–æ–Ω –∏—Å—Ç–∏–Ω–Ω—ã—Ö –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π: [{true_returns.min():.4f}, {true_returns.max():.4f}]\")\n",
    "    print(f\"–î–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π: [{predicted_returns.min():.4f}, {predicted_returns.max():.4f}]\")\n",
    "    \n",
    "    # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–º–µ—â–µ–Ω–∏—è\n",
    "    bias = np.mean(true_returns) - np.mean(predicted_returns)\n",
    "    predicted_returns_corrected = predicted_returns + bias\n",
    "    \n",
    "    print(f\"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Å–º–µ—â–µ–Ω–∏–µ: {bias:.6f}\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–æ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏: {np.mean(predicted_returns):.6f}\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–æ—Å–ª–µ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏: {np.mean(predicted_returns_corrected):.6f}\")\n",
    "    \n",
    "    # –î–ª—è Brier score - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
    "    min_ret, max_ret = predicted_returns_corrected.min(), predicted_returns_corrected.max()\n",
    "    if max_ret > min_ret:\n",
    "        normalized_returns = (predicted_returns_corrected - min_ret) / (max_ret - min_ret)\n",
    "    else:\n",
    "        normalized_returns = np.full_like(predicted_returns_corrected, 0.5)\n",
    "    \n",
    "    probabilities = normalized_returns\n",
    "    \n",
    "    # –ë–µ–π–∑–ª–∞–π–Ω—ã\n",
    "    base_mae = np.mean(np.abs(true_returns - np.mean(true_returns)))\n",
    "    base_brier = 0.25\n",
    "    \n",
    "    print(f\"–ë–µ–π–∑–ª–∞–π–Ω MAE: {base_mae:.6f}\")\n",
    "    print(f\"–ë–µ–π–∑–ª–∞–π–Ω Brier: {base_brier:.4f}\")\n",
    "    print(f\"MAE –º–æ–¥–µ–ª–∏ (–¥–æ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏): {mean_absolute_error(true_returns, predicted_returns):.6f}\")\n",
    "    print(f\"MAE –º–æ–¥–µ–ª–∏ (–ø–æ—Å–ª–µ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏): {mean_absolute_error(true_returns, predicted_returns_corrected):.6f}\")\n",
    "    \n",
    "    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫\n",
    "    metrics = calculate_competition_metrics(\n",
    "        y_true=true_returns,\n",
    "        y_pred=predicted_returns_corrected,\n",
    "        probabilities=probabilities,\n",
    "        base_mae=base_mae,\n",
    "        base_brier=base_brier\n",
    "    )\n",
    "    \n",
    "    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    print(\"\\n=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ú–ï–¢–†–ò–ö ===\")\n",
    "    print(f\"MAE: {metrics['MAE']:.6f}\")\n",
    "    print(f\"–ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π MAE: {metrics['MAE_norm']:.4f}\")\n",
    "    if metrics['Brier'] is not None:\n",
    "        print(f\"Brier Score: {metrics['Brier']:.6f}\")\n",
    "    print(f\"–ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Brier: {metrics['Brier_norm']:.4f}\")\n",
    "    print(f\"Directional Accuracy: {metrics['DA']:.4f}\")\n",
    "    print(f\"–§–∏–Ω–∞–ª—å–Ω—ã–π Score: {metrics['Final_Score']:.4f}\")\n",
    "    \n",
    "    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞\n",
    "    print(f\"\\n=== –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê ===\")\n",
    "    print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π: {len(true_returns)}\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω—è—è –∏—Å—Ç–∏–Ω–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {np.mean(true_returns):.6f}\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω—è—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å (–¥–æ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏): {np.mean(predicted_returns):.6f}\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω—è—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å (–ø–æ—Å–ª–µ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏): {np.mean(predicted_returns_corrected):.6f}\")\n",
    "    print(f\"–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {metrics['DA']:.2%}\")\n",
    "    \n",
    "    return metrics, predicted_returns_corrected, true_returns\n",
    "\n",
    "# –§–£–ù–ö–¶–ò–Ø –°–û–ó–î–ê–ù–ò–Ø –ü–†–ò–ó–ù–ê–ö–û–í\n",
    "def create_advanced_momentum_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # –û—Å–Ω–æ–≤–Ω—ã–µ —Ü–µ–Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "    if 'close' in df.columns:\n",
    "        # –†–∞–∑–Ω–æ—Å—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤–º–µ—Å—Ç–æ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π \n",
    "        for period in [1, 2, 3]:\n",
    "            df[f'price_diff_{period}'] = df['close'].diff(period)\n",
    "            df[f'price_change_{period}'] = df['close'].pct_change(period)\n",
    "        \n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è\n",
    "        rolling_std = df['close'].pct_change().rolling(10).std()\n",
    "        df['normalized_change'] = df['close'].pct_change() / (rolling_std + 1e-8)\n",
    "        \n",
    "        # –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "        for window in [3, 5, 8]:\n",
    "            sma = df['close'].rolling(window).mean()\n",
    "            df[f'trend_{window}'] = (df['close'] - sma) / sma\n",
    "            df[f'momentum_{window}'] = df['close'] / df['close'].shift(window) - 1\n",
    "            \n",
    "            # –£—Å–∫–æ—Ä–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞\n",
    "            if window > 3:\n",
    "                df[f'acceleration_{window}'] = df[f'trend_{window}'] - df[f'trend_{window}'].shift(1)\n",
    "    \n",
    "    # –û–±—ä–µ–º–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π\n",
    "    if 'volume' in df.columns:\n",
    "        volume_sma = df['volume'].rolling(10).mean()\n",
    "        volume_std = df['volume'].rolling(10).std()\n",
    "        df['volume_ratio'] = df['volume'] / (volume_sma + 1e-8)\n",
    "        df['volume_zscore'] = (df['volume'] - volume_sma) / (volume_std + 1e-8)\n",
    "        \n",
    "        # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –æ–±—ä–µ–º–∞ –∏ —Ü–µ–Ω—ã\n",
    "        if 'close' in df.columns:\n",
    "            price_change = df['close'].pct_change()\n",
    "            df['volume_price_corr'] = price_change.rolling(5).corr(df['volume'].pct_change())\n",
    "    \n",
    "    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π\n",
    "    if 'rsi' in df.columns:\n",
    "        df['rsi_normalized'] = (df['rsi'] - 50) / 30  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–æ–∫—Ä—É–≥ 50\n",
    "        df['rsi_signal'] = np.where(df['rsi'] > 65, 1, np.where(df['rsi'] < 35, -1, 0))\n",
    "    \n",
    "    if 'macd' in df.columns:\n",
    "        macd_std = df['macd'].rolling(20).std()\n",
    "        df['macd_normalized'] = df['macd'] / (macd_std + 1e-8)\n",
    "        df['macd_signal'] = np.sign(df['macd'])\n",
    "    \n",
    "    # –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π\n",
    "    news_cols = [col for col in df.columns if 'news' in col or 'sentiment' in col]\n",
    "    for col in news_cols:\n",
    "        if df[col].dtype in [np.int64, np.float64]:\n",
    "            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "            col_mean = df[col].rolling(10).mean()\n",
    "            col_std = df[col].rolling(10).std()\n",
    "            df[f'{col}_normalized'] = (df[col] - col_mean) / (col_std + 1e-8)\n",
    "            df[f'{col}_trend'] = df[col] / col_mean - 1\n",
    "    \n",
    "    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "    if 'begin' in df.columns:\n",
    "        df['day_of_week_sin'] = np.sin(2 * np.pi * df['begin'].dt.dayofweek / 7)\n",
    "        df['day_of_week_cos'] = np.cos(2 * np.pi * df['begin'].dt.dayofweek / 7)\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['begin'].dt.month / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['begin'].dt.month / 12)\n",
    "    \n",
    "    # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã\n",
    "    if 'close' in df.columns:\n",
    "        returns = df['close'].pct_change()\n",
    "        df['volatility_regime'] = returns.rolling(10).std()\n",
    "        df['high_volatility'] = (df['volatility_regime'] > df['volatility_regime'].quantile(0.7)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# –ú–û–î–ï–õ–¨ –° –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–û–ô –í–´–ë–û–†–ö–ò\n",
    "def create_balanced_return_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π —Å–º–µ—â–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "    \"\"\"\n",
    "    models = [\n",
    "        ('rf', RandomForestRegressor(\n",
    "            n_estimators=100, \n",
    "            max_depth=7,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=42\n",
    "        )),\n",
    "        ('gb', HistGradientBoostingRegressor(\n",
    "            max_iter=100,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42\n",
    "        )),\n",
    "        ('ridge', Ridge(alpha=0.5, random_state=42)),\n",
    "        ('et', ExtraTreesRegressor(\n",
    "            n_estimators=80,\n",
    "            max_depth=6,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ]\n",
    "    \n",
    "    # –ê–Ω—Å–∞–º–±–ª—å —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π\n",
    "    ensemble = VotingRegressor(estimators=models, weights=[3, 2, 1, 2])\n",
    "    \n",
    "    return ensemble\n",
    "\n",
    "# –ü–ê–ô–ü–õ–ê–ô–ù –° –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¶–ò–ï–ô –ò –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–û–ô\n",
    "def regularized_return_pipeline(train_df, test_df):\n",
    "    \"\"\"\n",
    "    –ü–∞–π–ø–ª–∞–π–Ω —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Å–º–µ—â–µ–Ω–∏—è\n",
    "    \"\"\"\n",
    "    print(\"–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...\")\n",
    "    train_df = create_advanced_momentum_features(train_df)\n",
    "    test_df = create_advanced_momentum_features(test_df)\n",
    "    \n",
    "    # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
    "    train_df = train_df.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "    test_df = test_df.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "    \n",
    "    # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è - –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –≤—ã–±—Ä–æ—Å–æ–≤\n",
    "    if 'close' in train_df.columns:\n",
    "        future_returns = train_df['close'].shift(-1) / train_df['close'] - 1\n",
    "        \n",
    "        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–±—Ä–æ—Å—ã (–æ–±—Ä–µ–∑–∞–µ–º –Ω–∞ 5% –∏ 95% –∫–≤–∞–Ω—Ç–∏–ª—è—Ö)\n",
    "        lower_bound = future_returns.quantile(0.05)\n",
    "        upper_bound = future_returns.quantile(0.95)\n",
    "        train_df['target_return'] = future_returns.clip(lower_bound, upper_bound)\n",
    "    \n",
    "    # –û—Ç–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "    feature_candidates = []\n",
    "    \n",
    "    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    priority_categories = [\n",
    "        'normalized', 'trend', 'momentum', 'diff', 'ratio', 'zscore',\n",
    "        'signal', 'volatility', 'acceleration', 'corr'\n",
    "    ]\n",
    "    \n",
    "    for col in train_df.columns:\n",
    "        if (col not in ['ticker', 'begin', 'open', 'high', 'low', 'close', 'volume', 'target_return'] and\n",
    "            train_df[col].dtype in [np.int64, np.float64]):\n",
    "            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –∏ —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "            if any(keyword in col for keyword in priority_categories):\n",
    "                feature_candidates.append(col)\n",
    "    \n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "    other_features = [col for col in train_df.columns \n",
    "                     if (col not in ['ticker', 'begin', 'open', 'high', 'low', 'close', 'volume', 'target_return'] + feature_candidates and\n",
    "                         train_df[col].dtype in [np.int64, np.float64])]\n",
    "    \n",
    "    feature_columns = feature_candidates + other_features[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ\n",
    "    \n",
    "    print(f\"–û—Ç–æ–±—Ä–∞–Ω–æ {len(feature_columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
    "    print(f\"–õ—É—á—à–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {feature_columns[:12]}\")\n",
    "    \n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    if 'target_return' in train_df.columns:\n",
    "        train_data = train_df[:-1].copy()  # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É\n",
    "        X_train = train_data[feature_columns]\n",
    "        y_train = train_data['target_return']\n",
    "        \n",
    "        # –¢—â–∞—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "        valid_mask = ~y_train.isnull() & ~X_train.isnull().any(axis=1)\n",
    "        X_train = X_train[valid_mask]\n",
    "        y_train = y_train[valid_mask]\n",
    "        \n",
    "        if len(X_train) > 8:\n",
    "            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Å RobustScaler –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ –≤—ã–±—Ä–æ—Å–∞–º\n",
    "            scaler = RobustScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            \n",
    "            # –û–±—É—á–µ–Ω–∏–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "            print(\"–û–±—É—á–µ–Ω–∏–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...\")\n",
    "            model = create_balanced_return_model(X_train_scaled, y_train)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π\n",
    "            X_test = test_df[feature_columns]\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            raw_predictions = model.predict(X_test_scaled)\n",
    "            \n",
    "            # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π - –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "            prediction_std = np.std(raw_predictions)\n",
    "            prediction_mean = np.mean(raw_predictions)\n",
    "            \n",
    "            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 2 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π\n",
    "            capped_predictions = np.clip(\n",
    "                raw_predictions, \n",
    "                prediction_mean - 2 * prediction_std,\n",
    "                prediction_mean + 2 * prediction_std\n",
    "            )\n",
    "            \n",
    "            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ\n",
    "            smoothed_predictions = 0.7 * capped_predictions + 0.3 * prediction_mean\n",
    "            \n",
    "            print(f\"–°—ã—Ä—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: [{raw_predictions.min():.4f}, {raw_predictions.max():.4f}]\")\n",
    "            print(f\"–†–µ–≥—É–ª—è—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ: [{smoothed_predictions.min():.4f}, {smoothed_predictions.max():.4f}]\")\n",
    "            print(f\"–°—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {np.mean(smoothed_predictions):.6f}\")\n",
    "            \n",
    "            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ü–µ–Ω—ã\n",
    "            last_train_price = train_df['close'].iloc[-1]\n",
    "            predicted_prices = last_train_price * (1 + smoothed_predictions)\n",
    "            \n",
    "            return predicted_prices, smoothed_predictions\n",
    "        else:\n",
    "            print(\"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\")\n",
    "            return None, None\n",
    "    else:\n",
    "        print(\"–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é\")\n",
    "        return None, None\n",
    "\n",
    "# –§–ò–ù–ê–õ–¨–ù–´–ô –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù\n",
    "def optimized_final_pipeline(train_df, test_df):\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"–ó–ê–ü–£–°–ö –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û –§–ò–ù–ê–õ–¨–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π\n",
    "    return_prices, return_predictions = regularized_return_pipeline(train_df.copy(), test_df.copy())\n",
    "    \n",
    "    if return_prices is None:\n",
    "        print(\"–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback...\")\n",
    "        # Fallback: –ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ü–µ–Ω\n",
    "        last_price = train_df['close'].iloc[-1]\n",
    "        fallback_predictions = np.full(len(test_df), last_price)\n",
    "        return_prices = fallback_predictions\n",
    "        return_predictions = np.zeros(len(test_df))\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "    final_predictions = {}\n",
    "    final_predictions['close'] = return_prices\n",
    "    \n",
    "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥—Ä—É–≥–∏—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π\n",
    "    if 'close' in final_predictions:\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö\n",
    "        recent_train = train_df.tail(10)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Ç–æ—á–µ–∫\n",
    "        \n",
    "        for col in ['open', 'high', 'low']:\n",
    "            if col in recent_train.columns:\n",
    "                # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ–¥–∞–≤–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "                ratios = recent_train[col] / recent_train['close']\n",
    "                current_ratio = ratios.mean()\n",
    "                \n",
    "                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —à—É–º –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è\n",
    "                noise = np.random.normal(0, 0.001, len(return_prices))\n",
    "                final_predictions[col] = return_prices * (current_ratio + noise)\n",
    "                \n",
    "                print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ {col} (ratio {current_ratio:.4f})\")\n",
    "        \n",
    "        # Volume –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
    "        if 'volume' in train_df.columns:\n",
    "            # –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å: —Å—Ä–µ–¥–Ω–∏–π –æ–±—ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–Ω–µ–π + —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å\n",
    "            recent_volume = train_df['volume'].tail(5).mean()\n",
    "            day_of_week = test_df['begin'].dt.dayofweek if 'begin' in test_df.columns else 0\n",
    "            \n",
    "            # –£—á–µ—Ç –¥–Ω—è –Ω–µ–¥–µ–ª–∏ (–µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã)\n",
    "            if 'begin' in train_df.columns:\n",
    "                weekday_pattern = train_df.groupby(train_df['begin'].dt.dayofweek)['volume'].mean()\n",
    "                volume_multipliers = weekday_pattern / weekday_pattern.mean()\n",
    "                \n",
    "                # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –¥–Ω—è –Ω–µ–¥–µ–ª–∏\n",
    "                base_volume = recent_volume\n",
    "                predicted_volumes = []\n",
    "                for i, date in enumerate(test_df['begin']):\n",
    "                    weekday = date.dayofweek\n",
    "                    multiplier = volume_multipliers.get(weekday, 1.0)\n",
    "                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —Å–ª—É—á–∞–π–Ω—ã–π —à—É–º\n",
    "                    noise = np.random.normal(0, 0.1)\n",
    "                    predicted_volumes.append(base_volume * multiplier * (1 + noise))\n",
    "                \n",
    "                final_predictions['volume'] = np.array(predicted_volumes)\n",
    "            else:\n",
    "                final_predictions['volume'] = np.full(len(test_df), recent_volume)\n",
    "    \n",
    "    return final_predictions, return_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "588f729e-06d6-4f8f-8fdd-9de84e361c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–ê–ü–£–°–ö –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê\n",
      "============================================================\n",
      "–†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: (25, 31)\n",
      "–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: (25, 31)\n",
      "============================================================\n",
      "–ó–ê–ü–£–°–ö –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û –§–ò–ù–ê–õ–¨–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê\n",
      "============================================================\n",
      "–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...\n",
      "–û—Ç–æ–±—Ä–∞–Ω–æ 51 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
      "–õ—É—á—à–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: ['price_diff_1', 'price_diff_2', 'price_diff_3', 'normalized_change', 'trend_3', 'momentum_3', 'trend_5', 'momentum_5', 'acceleration_5', 'trend_8', 'momentum_8', 'acceleration_8']\n",
      "–û–±—É—á–µ–Ω–∏–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...\n",
      "–°—ã—Ä—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: [-0.0070, 0.0192]\n",
      "–†–µ–≥—É–ª—è—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ: [-0.0034, 0.0150]\n",
      "–°—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: 0.005018\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ open (ratio 0.9968)\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ high (ratio 1.0056)\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ low (ratio 0.9906)\n",
      "\n",
      "============================================================\n",
      "–û–¶–ï–ù–ö–ê –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ô –ú–û–î–ï–õ–ò\n",
      "============================================================\n",
      "\n",
      "=== –û–¶–ï–ù–ö–ê –ü–û –í–°–ï–ú –¶–ï–õ–ï–í–´–ú –ü–ï–†–ï–ú–ï–ù–ù–´–ú ===\n",
      "\n",
      "OPEN:\n",
      "  MAE: 34.4907\n",
      "  RMSE: 39.4279\n",
      "  MAPE: 3.61%\n",
      "\n",
      "HIGH:\n",
      "  MAE: 33.6252\n",
      "  RMSE: 38.4041\n",
      "  MAPE: 3.49%\n",
      "\n",
      "LOW:\n",
      "  MAE: 31.5677\n",
      "  RMSE: 36.1980\n",
      "  MAPE: 3.33%\n",
      "\n",
      "CLOSE:\n",
      "  MAE: 31.4869\n",
      "  RMSE: 36.6658\n",
      "  MAPE: 3.29%\n",
      "\n",
      "VOLUME:\n",
      "  MAE: 267810.6040\n",
      "  RMSE: 360012.9630\n",
      "  MAPE: 27.02%\n",
      "\n",
      "=== –û–¶–ï–ù–ö–ê –ê–ù–°–ê–ú–ë–õ–Ø –ù–ê –ì–û–†–ò–ó–û–ù–¢–ï 1 –î–ù–ï–ô ===\n",
      "–î–∏–∞–ø–∞–∑–æ–Ω –∏—Å—Ç–∏–Ω–Ω—ã—Ö –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π: [-0.0184, 0.0309]\n",
      "–î–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π: [-0.0689, 0.0025]\n",
      "–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Å–º–µ—â–µ–Ω–∏–µ: 0.034075\n",
      "–°—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–æ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏: -0.033484\n",
      "–°—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–æ—Å–ª–µ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏: 0.000591\n",
      "–ë–µ–π–∑–ª–∞–π–Ω MAE: 0.008336\n",
      "–ë–µ–π–∑–ª–∞–π–Ω Brier: 0.2500\n",
      "MAE –º–æ–¥–µ–ª–∏ (–¥–æ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏): 0.034395\n",
      "MAE –º–æ–¥–µ–ª–∏ (–ø–æ—Å–ª–µ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏): 0.016356\n",
      "\n",
      "=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ú–ï–¢–†–ò–ö ===\n",
      "MAE: 0.016356\n",
      "–ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π MAE: 0.0000\n",
      "Brier Score: 0.207515\n",
      "–ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Brier: 0.1699\n",
      "Directional Accuracy: 0.5833\n",
      "–§–∏–Ω–∞–ª—å–Ω—ã–π Score: 0.1093\n",
      "\n",
      "=== –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê ===\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π: 24\n",
      "–°—Ä–µ–¥–Ω—è—è –∏—Å—Ç–∏–Ω–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: 0.000591\n",
      "–°—Ä–µ–¥–Ω—è—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å (–¥–æ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏): -0.033484\n",
      "–°—Ä–µ–¥–Ω—è—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å (–ø–æ—Å–ª–µ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏): 0.000591\n",
      "–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π: 58.33%\n",
      "\n",
      "üéØ –§–∏–Ω–∞–ª—å–Ω—ã–π score –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: 0.1093\n",
      "\n",
      "=== –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó ===\n",
      "Directional Accuracy: 58.3%\n",
      "MAE –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π: 0.016356\n",
      "–ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π MAE: 0.0000\n",
      "–ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Brier: 0.1699\n",
      "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: 21/25 (84.0%)\n",
      "‚úÖ –ó–ù–ê–ß–ò–¢–ï–õ–¨–ù–û–ï –£–õ–£–ß–®–ï–ù–ò–ï!\n",
      "\n",
      "=== –ö–û–ù–ö–†–ï–¢–ù–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò ===\n",
      "‚Ä¢ –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞: —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–µ–ª–∏—á–∏–Ω –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π\n",
      "‚Ä¢ –†–µ—à–µ–Ω–∏–µ: —É–ª—É—á—à–∏—Ç–µ feature engineering –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é\n",
      "\n",
      "==================================================\n",
      "–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù –ó–ê–í–ï–†–®–ï–ù!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# –û–°–ù–û–í–ù–û–ô –ë–õ–û–ö –í–´–ü–û–õ–ù–ï–ù–ò–Ø\n",
    "print(\"–ó–ê–ü–£–°–ö –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞—à–∏ –¥–∞–Ω–Ω—ã–µ\n",
    "train_df = result[:25].copy()\n",
    "test_df = result[25:].copy()\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞—Ç\n",
    "train_df['begin'] = pd.to_datetime(train_df['begin'])\n",
    "test_df['begin'] = pd.to_datetime(test_df['begin'])\n",
    "\n",
    "# –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞\n",
    "train_df = train_df.sort_values('begin')\n",
    "test_df = test_df.sort_values('begin')\n",
    "\n",
    "print(f\"–†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {train_df.shape}\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {test_df.shape}\")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω\n",
    "final_predictions, return_predictions = optimized_final_pipeline(train_df, test_df)\n",
    "\n",
    "if final_predictions and 'close' in final_predictions:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"–û–¶–ï–ù–ö–ê –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ô –ú–û–î–ï–õ–ò\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # –û—Ü–µ–Ω–∫–∞ –ø–æ –≤—Å–µ–º —Ü–µ–ª–µ–≤—ã–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º\n",
    "    basic_metrics = evaluate_all_targets(test_df, final_predictions)\n",
    "    \n",
    "    # –û—Ü–µ–Ω–∫–∞ –¥–ª—è —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è\n",
    "    competition_metrics, corrected_returns, true_returns = evaluate_ensemble_with_metrics(\n",
    "        test_df, final_predictions, horizon=1\n",
    "    )\n",
    "    '''\n",
    "        !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        \n",
    "        –ü–ï–†–ï–ú–ï–ù–ù–ê–Ø horizon –û–¢–í–ï–ß–ê–ï–¢ –ó–ê –ö–û–õ–ò–ß–ï–°–¢–í–û –î–ù–ï–ô –í –ü–†–û–ì–ù–û–ó–ï\n",
    "\n",
    "        !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    '''\n",
    "    \n",
    "    print(f\"\\nüéØ –§–∏–Ω–∞–ª—å–Ω—ã–π score –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {competition_metrics['Final_Score']:.4f}\")\n",
    "    \n",
    "    # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
    "    print(f\"\\n=== –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó ===\")\n",
    "    print(f\"Directional Accuracy: {competition_metrics['DA']:.1%}\")\n",
    "    print(f\"MAE –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π: {competition_metrics['MAE']:.6f}\")\n",
    "    print(f\"–ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π MAE: {competition_metrics['MAE_norm']:.4f}\")\n",
    "    print(f\"–ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Brier: {competition_metrics['Brier_norm']:.4f}\")\n",
    "    \n",
    "    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "    positive_predictions = np.sum(return_predictions > 0)\n",
    "    total_predictions = len(return_predictions)\n",
    "    print(f\"–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {positive_predictions}/{total_predictions} ({positive_predictions/total_predictions:.1%})\")\n",
    "    \n",
    "    # –û—Ü–µ–Ω–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è\n",
    "    if competition_metrics['Final_Score'] > 0.1:\n",
    "        print(\"‚úÖ –ó–ù–ê–ß–ò–¢–ï–õ–¨–ù–û–ï –£–õ–£–ß–®–ï–ù–ò–ï!\")\n",
    "    elif competition_metrics['Final_Score'] > 0.08:\n",
    "        print(\"‚ö†Ô∏è  –£–ú–ï–†–ï–ù–ù–û–ï –£–õ–£–ß–®–ï–ù–ò–ï\")\n",
    "    else:\n",
    "        print(\"‚ùå –¢–†–ï–ë–£–Æ–¢–°–Ø –î–ê–õ–¨–ù–ï–ô–®–ò–ï –£–õ–£–ß–®–ï–ù–ò–Ø\")\n",
    "        \n",
    "    # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
    "    print(f\"\\n=== –ö–û–ù–ö–†–ï–¢–ù–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò ===\")\n",
    "    if competition_metrics['MAE_norm'] < 0.3:\n",
    "        print(\"‚Ä¢ –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞: —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–µ–ª–∏—á–∏–Ω –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π\")\n",
    "        print(\"‚Ä¢ –†–µ—à–µ–Ω–∏–µ: —É–ª—É—á—à–∏—Ç–µ feature engineering –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é\")\n",
    "    \n",
    "    if positive_predictions / total_predictions < 0.3:\n",
    "        print(\"‚Ä¢ –ü—Ä–æ–±–ª–µ–º–∞: –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π\")\n",
    "        print(\"‚Ä¢ –†–µ—à–µ–Ω–∏–µ: –¥–æ–±–∞–≤—å—Ç–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é —Å–º–µ—â–µ–Ω–∏—è\")\n",
    "    \n",
    "    if competition_metrics['DA'] > 0.6:\n",
    "        print(f\"‚Ä¢ –û—Ç–ª–∏—á–Ω–æ! Directional Accuracy {competition_metrics['DA']:.1%} - –º–æ–¥–µ–ª—å —Ö–æ—Ä–æ—à–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ\")\n",
    "\n",
    "else:\n",
    "    print(\"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù –ó–ê–í–ï–†–®–ï–ù!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e18c5f8-0323-4f6f-9440-e1bc671caf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'close': array([911.58705616, 911.94918325, 910.63826373, 912.1255027 ,\n",
       "        911.59772477, 913.0926152 , 912.72753673, 911.08886659,\n",
       "        912.18766938, 912.35698594, 908.62040021, 906.50655185,\n",
       "        907.34444451, 908.6796289 , 909.74501828, 923.21829644,\n",
       "        918.93211112, 919.59766252, 919.87542555, 919.1949952 ,\n",
       "        918.84035735, 919.96567287, 918.62613613, 917.96068335,\n",
       "        917.64641222]),\n",
       " 'open': array([908.79477364, 907.95844439, 908.55539776, 909.44118242,\n",
       "        911.16101156, 910.56067376, 909.05452875, 907.13319153,\n",
       "        910.4333802 , 909.59071808, 906.91572597, 902.83464139,\n",
       "        904.41542006, 906.46981881, 907.03572301, 920.16872149,\n",
       "        914.23009282, 916.63420359, 915.15930169, 915.24724133,\n",
       "        916.28391776, 917.92871198, 915.07080903, 915.26188832,\n",
       "        914.74612051]),\n",
       " 'high': array([916.95774633, 917.29494467, 916.42532429, 917.79367079,\n",
       "        916.10272739, 918.40123722, 916.90483176, 915.84399634,\n",
       "        916.20122885, 917.92372488, 914.70176583, 912.78073559,\n",
       "        912.22859048, 912.60087169, 914.09784234, 927.53478109,\n",
       "        924.03818601, 925.57172886, 923.5678654 , 923.55144731,\n",
       "        922.94439075, 923.97427299, 923.38362734, 924.29928611,\n",
       "        923.12148071]),\n",
       " 'low': array([902.29961402, 903.32491234, 901.46234235, 903.90377749,\n",
       "        902.90717253, 905.6824293 , 903.7117582 , 901.28780846,\n",
       "        903.08580293, 903.04931386, 900.09342413, 897.45473127,\n",
       "        899.19604405, 898.31339726, 901.43931393, 913.60798677,\n",
       "        909.12275637, 910.31249792, 911.06445566, 912.87151936,\n",
       "        910.94263432, 911.16593516, 910.08663227, 910.27143941,\n",
       "        909.14740312]),\n",
       " 'volume': array([ 799447.80383087,  790144.57741598,  896047.95186926,\n",
       "         900473.85614539,  860743.74142342, 1029517.77245545,\n",
       "         730726.05549865,  875321.1908193 ,  986535.97104038,\n",
       "         843566.78978876,  999552.87027201,  664390.43502305,\n",
       "         868184.41954554,  853315.66278084,  912831.50834829,\n",
       "         774080.26557606,  691049.80942544,  793445.92315981,\n",
       "         767113.24955975, 1020495.82440364,  931448.64079172,\n",
       "         684757.00395128,  759279.12086159,  788526.85283745,\n",
       "         993314.04735052])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e7bebda-8026-43bd-87b1-dbedd3ab5904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00218454,  0.00258266,  0.00114145,  0.0027765 ,  0.00219627,\n",
       "        0.00383973,  0.00343836,  0.00163684,  0.00284484,  0.00303099,\n",
       "       -0.00107696, -0.00340089, -0.00247972, -0.00101184,  0.00015943,\n",
       "        0.01497174,  0.01025958,  0.01099127,  0.01129664,  0.01054859,\n",
       "        0.0101587 ,  0.01139586,  0.00992319,  0.0091916 ,  0.0088461 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51140df1-f5d4-413e-9476-40078c4f0f61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
