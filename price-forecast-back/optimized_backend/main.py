from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import pandas as pd
import numpy as np
import joblib
import os
import json
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –º–æ–¥–µ–ª–µ–π
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import mean_absolute_error

app = FastAPI(title="Optimized Price Prediction API", version="2.0.0")

# –ú–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã
model = None
scaler = None
feature_columns = []

# –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
target_columns = ['open', 'high', 'low', 'close', 'volume']

class PredictionRequest(BaseModel):
    params: List[Dict[str, Any]]

class PredictionResponse(BaseModel):
    predictions: List[Dict[str, Any]]

class TrainingConfig(BaseModel):
    train_size: Optional[int] = 25
    test_size: Optional[int] = None

class TrainingResponse(BaseModel):
    status: str
    message: str
    final_score: float
    feature_count: int

def calculate_competition_metrics(y_true, y_pred, probabilities=None, base_mae=None, base_brier=None):
    """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è"""
    metrics = {}
    
    # 1. MAE (Mean Absolute Error)
    mae = mean_absolute_error(y_true, y_pred)
    metrics['MAE'] = mae
    
    # –ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π MAE (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–µ–π–∑–ª–∞–π–Ω–∞)
    if base_mae is not None and base_mae > 0:
        mae_norm = max(0, 1 - (mae / base_mae))
        metrics['MAE_norm'] = mae_norm
    else:
        metrics['MAE_norm'] = 0.0
    
    # 2. Brier Score (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏)
    if probabilities is not None:
        # –°–æ–∑–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏: 1 –µ—Å–ª–∏ —Ä–æ—Å—Ç, 0 –µ—Å–ª–∏ –ø–∞–¥–µ–Ω–∏–µ
        true_directions = (y_true > 0).astype(int)
        brier = np.mean((true_directions - probabilities) ** 2)
        metrics['Brier'] = brier
        
        # –ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Brier
        if base_brier is not None and base_brier > 0:
            brier_norm = max(0, 1 - (brier / base_brier))
            metrics['Brier_norm'] = brier_norm
        else:
            metrics['Brier_norm'] = 0.0
    else:
        metrics['Brier'] = None
        metrics['Brier_norm'] = 0.0
    
    # 3. Directional Accuracy (DA)
    true_sign = np.sign(y_true)
    pred_sign = np.sign(y_pred)
    da = np.mean(true_sign == pred_sign)
    metrics['DA'] = da
    
    # 4. –ò—Ç–æ–≥–æ–≤—ã–π Score
    score_components = []
    score_components.append(0.7 * metrics['MAE_norm'])
    score_components.append(0.3 * metrics['Brier_norm'])
    score_components.append(0.1 * metrics['DA'])
    
    metrics['Final_Score'] = sum(score_components)
    
    return metrics

def calculate_returns(prices, horizon=1):
    """–†–∞—Å—á–µ—Ç –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –Ω–∞ –∑–∞–¥–∞–Ω–Ω–æ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–µ"""
    future_prices = prices.shift(-horizon)
    returns = (future_prices / prices) - 1
    return returns

def create_advanced_momentum_features(df):
    """–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    df = df.copy()
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ —Ü–µ–Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    if 'close' in df.columns:
        # –†–∞–∑–Ω–æ—Å—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤–º–µ—Å—Ç–æ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π 
        for period in [1, 2, 3]:
            df[f'price_diff_{period}'] = df['close'].diff(period)
            df[f'price_change_{period}'] = df['close'].pct_change(period)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        rolling_std = df['close'].pct_change().rolling(10).std()
        df['normalized_change'] = df['close'].pct_change() / (rolling_std + 1e-8)
        
        # –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for window in [3, 5, 8]:
            sma = df['close'].rolling(window).mean()
            df[f'trend_{window}'] = (df['close'] - sma) / sma
            df[f'momentum_{window}'] = df['close'] / df['close'].shift(window) - 1
            
            # –£—Å–∫–æ—Ä–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞
            if window > 3:
                df[f'acceleration_{window}'] = df[f'trend_{window}'] - df[f'trend_{window}'].shift(1)
    
    # –û–±—ä–µ–º–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
    if 'volume' in df.columns:
        volume_sma = df['volume'].rolling(10).mean()
        volume_std = df['volume'].rolling(10).std()
        df['volume_ratio'] = df['volume'] / (volume_sma + 1e-8)
        df['volume_zscore'] = (df['volume'] - volume_sma) / (volume_std + 1e-8)
        
        # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –æ–±—ä–µ–º–∞ –∏ —Ü–µ–Ω—ã
        if 'close' in df.columns:
            price_change = df['close'].pct_change()
            df['volume_price_corr'] = price_change.rolling(5).corr(df['volume'].pct_change())
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
    if 'rsi' in df.columns:
        df['rsi_normalized'] = (df['rsi'] - 50) / 30  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–æ–∫—Ä—É–≥ 50
        df['rsi_signal'] = np.where(df['rsi'] > 65, 1, np.where(df['rsi'] < 35, -1, 0))
    
    if 'macd' in df.columns:
        macd_std = df['macd'].rolling(20).std()
        df['macd_normalized'] = df['macd'] / (macd_std + 1e-8)
        df['macd_signal'] = np.sign(df['macd'])
    
    # –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
    news_cols = [col for col in df.columns if 'news' in col or 'sentiment' in col]
    for col in news_cols:
        if df[col].dtype in [np.int64, np.float64]:
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            col_mean = df[col].rolling(10).mean()
            col_std = df[col].rolling(10).std()
            df[f'{col}_normalized'] = (df[col] - col_mean) / (col_std + 1e-8)
            df[f'{col}_trend'] = df[col] / col_mean - 1
    
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    if 'begin' in df.columns:
        df['day_of_week_sin'] = np.sin(2 * np.pi * df['begin'].dt.dayofweek / 7)
        df['day_of_week_cos'] = np.cos(2 * np.pi * df['begin'].dt.dayofweek / 7)
        df['month_sin'] = np.sin(2 * np.pi * df['begin'].dt.month / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['begin'].dt.month / 12)
    
    # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
    if 'close' in df.columns:
        returns = df['close'].pct_change()
        df['volatility_regime'] = returns.rolling(10).std()
        df['high_volatility'] = (df['volatility_regime'] > df['volatility_regime'].quantile(0.7)).astype(int)
    
    return df

def create_balanced_return_model(X_train, y_train):
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π —Å–º–µ—â–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
    models = [
        ('rf', RandomForestRegressor(
            n_estimators=100, 
            max_depth=7,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42
        )),
        ('gb', HistGradientBoostingRegressor(
            max_iter=100,
            max_depth=4,
            learning_rate=0.1,
            random_state=42
        )),
        ('ridge', Ridge(alpha=0.5, random_state=42)),
        ('et', ExtraTreesRegressor(
            n_estimators=80,
            max_depth=6,
            random_state=42
        ))
    ]
    
    # –ê–Ω—Å–∞–º–±–ª—å —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
    ensemble = VotingRegressor(estimators=models, weights=[3, 2, 1, 2])
    
    return ensemble

def regularized_return_pipeline(train_df, test_df):
    """–ü–∞–π–ø–ª–∞–π–Ω —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Å–º–µ—â–µ–Ω–∏—è"""
    print("–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    train_df = create_advanced_momentum_features(train_df)
    test_df = create_advanced_momentum_features(test_df)
    
    # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
    train_df = train_df.fillna(method='ffill').fillna(method='bfill').fillna(0)
    test_df = test_df.fillna(method='ffill').fillna(method='bfill').fillna(0)
    
    # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è - –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –≤—ã–±—Ä–æ—Å–æ–≤
    if 'close' in train_df.columns:
        future_returns = train_df['close'].shift(-1) / train_df['close'] - 1
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–±—Ä–æ—Å—ã (–æ–±—Ä–µ–∑–∞–µ–º –Ω–∞ 5% –∏ 95% –∫–≤–∞–Ω—Ç–∏–ª—è—Ö)
        lower_bound = future_returns.quantile(0.05)
        upper_bound = future_returns.quantile(0.95)
        train_df['target_return'] = future_returns.clip(lower_bound, upper_bound)
    
    # –û—Ç–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    feature_candidates = []
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    priority_categories = [
        'normalized', 'trend', 'momentum', 'diff', 'ratio', 'zscore',
        'signal', 'volatility', 'acceleration', 'corr'
    ]
    
    for col in train_df.columns:
        if (col not in ['ticker', 'begin', 'open', 'high', 'low', 'close', 'volume', 'target_return'] and
            train_df[col].dtype in [np.int64, np.float64]):
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –∏ —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if any(keyword in col for keyword in priority_categories):
                feature_candidates.append(col)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    other_features = [col for col in train_df.columns 
                     if (col not in ['ticker', 'begin', 'open', 'high', 'low', 'close', 'volume', 'target_return'] + feature_candidates and
                         train_df[col].dtype in [np.int64, np.float64])]
    
    feature_columns = feature_candidates + other_features[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    
    print(f"–û—Ç–æ–±—Ä–∞–Ω–æ {len(feature_columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"–õ—É—á—à–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {feature_columns[:12]}")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    if 'target_return' in train_df.columns:
        train_data = train_df[:-1].copy()  # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É
        X_train = train_data[feature_columns]
        y_train = train_data['target_return']
        
        # –¢—â–∞—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        valid_mask = ~y_train.isnull() & ~X_train.isnull().any(axis=1)
        X_train = X_train[valid_mask]
        y_train = y_train[valid_mask]
        
        if len(X_train) > 8:
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Å RobustScaler –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ –≤—ã–±—Ä–æ—Å–∞–º
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            
            # –û–±—É—á–µ–Ω–∏–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            print("–û–±—É—á–µ–Ω–∏–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
            model = create_balanced_return_model(X_train_scaled, y_train)
            model.fit(X_train_scaled, y_train)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
            X_test = test_df[feature_columns]
            X_test_scaled = scaler.transform(X_test)
            raw_predictions = model.predict(X_test_scaled)
            
            # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π - –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            prediction_std = np.std(raw_predictions)
            prediction_mean = np.mean(raw_predictions)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 2 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π
            capped_predictions = np.clip(
                raw_predictions, 
                prediction_mean - 2 * prediction_std,
                prediction_mean + 2 * prediction_std
            )
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
            smoothed_predictions = 0.7 * capped_predictions + 0.3 * prediction_mean
            
            print(f"–°—ã—Ä—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: [{raw_predictions.min():.4f}, {raw_predictions.max():.4f}]")
            print(f"–†–µ–≥—É–ª—è—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ: [{smoothed_predictions.min():.4f}, {smoothed_predictions.max():.4f}]")
            print(f"–°—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {np.mean(smoothed_predictions):.6f}")
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ü–µ–Ω—ã
            last_train_price = train_df['close'].iloc[-1]
            predicted_prices = last_train_price * (1 + smoothed_predictions)
            
            return predicted_prices, smoothed_predictions, feature_columns, scaler, model
        else:
            print("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return None, None, None, None, None
    else:
        print("–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é")
        return None, None, None, None, None

def optimized_final_pipeline(train_df, test_df):
    """–§–∏–Ω–∞–ª—å–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω"""
    print("=" * 60)
    print("–ó–ê–ü–£–°–ö –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û –§–ò–ù–ê–õ–¨–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê")
    print("=" * 60)
    
    # –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π
    return_prices, return_predictions, feature_cols, scaler_obj, model_obj = regularized_return_pipeline(train_df.copy(), test_df.copy())
    
    if return_prices is None:
        print("–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback...")
        # Fallback: –ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ü–µ–Ω
        last_price = train_df['close'].iloc[-1]
        fallback_predictions = np.full(len(test_df), last_price)
        return_prices = fallback_predictions
        return_predictions = np.zeros(len(test_df))
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    final_predictions = {}
    final_predictions['close'] = return_prices
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥—Ä—É–≥–∏—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π
    if 'close' in final_predictions:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö
        recent_train = train_df.tail(10)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Ç–æ—á–µ–∫
        
        for col in ['open', 'high', 'low']:
            if col in recent_train.columns:
                # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ–¥–∞–≤–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö
                ratios = recent_train[col] / recent_train['close']
                current_ratio = ratios.mean()
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —à—É–º –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                noise = np.random.normal(0, 0.001, len(return_prices))
                final_predictions[col] = return_prices * (current_ratio + noise)
                
                print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ {col} (ratio {current_ratio:.4f})")
        
        # Volume –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        if 'volume' in train_df.columns:
            # –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å: —Å—Ä–µ–¥–Ω–∏–π –æ–±—ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–Ω–µ–π + —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å
            recent_volume = train_df['volume'].tail(5).mean()
            
            # –£—á–µ—Ç –¥–Ω—è –Ω–µ–¥–µ–ª–∏ (–µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã)
            if 'begin' in train_df.columns:
                weekday_pattern = train_df.groupby(train_df['begin'].dt.dayofweek)['volume'].mean()
                volume_multipliers = weekday_pattern / weekday_pattern.mean()
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –¥–Ω—è –Ω–µ–¥–µ–ª–∏
                base_volume = recent_volume
                predicted_volumes = []
                for i, date in enumerate(test_df['begin']):
                    weekday = date.dayofweek
                    multiplier = volume_multipliers.get(weekday, 1.0)
                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —Å–ª—É—á–∞–π–Ω—ã–π —à—É–º
                    noise = np.random.normal(0, 0.1)
                    predicted_volumes.append(base_volume * multiplier * (1 + noise))
                
                final_predictions['volume'] = np.array(predicted_volumes)
            else:
                final_predictions['volume'] = np.full(len(test_df), recent_volume)
    
    return final_predictions, return_predictions, feature_cols, scaler_obj, model_obj

def train_models_with_config(data_file: str, config: TrainingConfig) -> TrainingResponse:
    """–û–±—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    global model, scaler, feature_columns
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        if data_file.endswith('.csv'):
            data = pd.read_csv(data_file)
        elif data_file.endswith('.json'):
            data = pd.read_json(data_file)
        else:
            raise ValueError("–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        if config.test_size is None:
            train_df = data[:config.train_size]
            test_df = data[config.train_size:]
        else:
            train_df = data[:config.train_size]
            test_df = data[config.train_size:config.train_size + config.test_size]
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞—Ç
        if 'begin' in train_df.columns:
            train_df['begin'] = pd.to_datetime(train_df['begin'])
            test_df['begin'] = pd.to_datetime(test_df['begin'])
            train_df = train_df.sort_values('begin')
            test_df = test_df.sort_values('begin')
        
        print(f"–†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {train_df.shape}")
        print(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {test_df.shape}")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω
        final_predictions, return_predictions, feature_cols, scaler_obj, model_obj = optimized_final_pipeline(train_df, test_df)
        
        if final_predictions and 'close' in final_predictions:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã
            model = model_obj
            scaler = scaler_obj
            feature_columns = feature_cols
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –º–æ–¥–µ–ª–µ–π
            os.makedirs('models', exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            joblib.dump(model, 'models/optimized_model.pkl')
            joblib.dump(scaler, 'models/optimized_scaler.pkl')
            joblib.dump(feature_columns, 'models/feature_columns.pkl')
            
            # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
            print("\n" + "=" * 60)
            print("–û–¶–ï–ù–ö–ê –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ô –ú–û–î–ï–õ–ò")
            print("=" * 60)
            
            # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
            true_prices = test_df['close'].values
            true_returns = calculate_returns(pd.Series(true_prices), horizon=1)
            valid_indices = ~np.isnan(true_returns)
            true_returns = true_returns[valid_indices]
            
            predicted_prices = final_predictions['close']
            predicted_prices = predicted_prices[:len(true_returns)]
            current_prices = true_prices[:len(predicted_prices)]
            predicted_returns = (predicted_prices / current_prices) - 1
            
            # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–º–µ—â–µ–Ω–∏—è
            bias = np.mean(true_returns) - np.mean(predicted_returns)
            predicted_returns_corrected = predicted_returns + bias
            
            # –î–ª—è Brier score
            min_ret, max_ret = predicted_returns_corrected.min(), predicted_returns_corrected.max()
            if max_ret > min_ret:
                normalized_returns = (predicted_returns_corrected - min_ret) / (max_ret - min_ret)
            else:
                normalized_returns = np.full_like(predicted_returns_corrected, 0.5)
            
            # –ë–µ–π–∑–ª–∞–π–Ω—ã
            base_mae = np.mean(np.abs(true_returns - np.mean(true_returns)))
            base_brier = 0.25
            
            # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
            metrics = calculate_competition_metrics(
                y_true=true_returns,
                y_pred=predicted_returns_corrected,
                probabilities=normalized_returns,
                base_mae=base_mae,
                base_brier=base_brier
            )
            
            print(f"\nüéØ –§–∏–Ω–∞–ª—å–Ω—ã–π score –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {metrics['Final_Score']:.4f}")
            print(f"Directional Accuracy: {metrics['DA']:.1%}")
            print(f"MAE –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π: {metrics['MAE']:.6f}")
            print(f"–ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π MAE: {metrics['MAE_norm']:.4f}")
            print(f"–ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Brier: {metrics['Brier_norm']:.4f}")
            
            return TrainingResponse(
                status="success",
                message=f"–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ",
                final_score=metrics['Final_Score'],
                feature_count=len(feature_columns)
            )
        else:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
        
    except Exception as e:
        import traceback
        print(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
        print(f"Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {str(e)}")

def load_models():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    global model, scaler, feature_columns
    
    try:
        model_path = "models/optimized_model.pkl"
        scaler_path = "models/optimized_scaler.pkl"
        features_path = "models/feature_columns.pkl"
        
        if os.path.exists(model_path) and os.path.exists(scaler_path) and os.path.exists(features_path):
            print("–ó–∞–≥—Ä—É–∂–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
            model = joblib.load(model_path)
            scaler = joblib.load(scaler_path)
            feature_columns = joblib.load(features_path)
            print(f"–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(feature_columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            return True
        else:
            print(f"–§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {model_path}, {scaler_path}, {features_path}")
            return False
        
    except Exception as e:
        import traceback
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        print(f"Traceback: {traceback.format_exc()}")
        return False

def predict_single_candle(input_data: Dict[str, Any]) -> Dict[str, float]:
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ–¥–Ω–æ–π —Å–≤–µ—á–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ DataFrame
        df = pd.DataFrame([input_data])
        
        # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df = create_advanced_momentum_features(df)
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
        missing_features = set(feature_columns) - set(df.columns)
        if missing_features:
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            for feature in missing_features:
                df[feature] = 0.0
        
        X = df[feature_columns]
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions = {}
        
        # –û—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –¥–ª—è close
        if model is not None and scaler is not None:
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
            X_scaled = scaler.transform(X)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
            return_prediction = model.predict(X_scaled)[0]
            
            # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            return_prediction = np.clip(return_prediction, -0.1, 0.1)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ü–µ–Ω—É –∑–∞–∫—Ä—ã—Ç–∏—è
            current_close = input_data.get('close', 100.0)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é —Ü–µ–Ω—É –∫–∞–∫ –±–∞–∑—É
            predicted_close = current_close * (1 + return_prediction)
            predictions['close'] = float(predicted_close)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥—Ä—É–≥–∏—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–π
            recent_ratios = {
                'open': 0.9968,   # –ü—Ä–∏–º–µ—Ä–Ω—ã–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
                'high': 1.0056,
                'low': 0.9906
            }
            
            for col in ['open', 'high', 'low']:
                if col in recent_ratios:
                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —à—É–º –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                    noise = np.random.normal(0, 0.001)
                    predictions[col] = float(predicted_close * (recent_ratios[col] + noise))
            
            # Volume –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            current_volume = input_data.get('volume', 1000000)
            # –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å: —Å—Ä–µ–¥–Ω–∏–π –æ–±—ä–µ–º + –Ω–µ–±–æ–ª—å—à–æ–π —à—É–º
            volume_noise = np.random.normal(0, 0.1)
            predictions['volume'] = float(current_volume * (1 + volume_noise))
        
        return predictions
        
    except Exception as e:
        import traceback
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
        print(f"Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}")

def generate_20_candles_from_history(input_candles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è 20 —Å–≤–µ—á–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö —Å–≤–µ—á–µ–π"""
    if not input_candles:
        raise ValueError("–ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –≤—Ö–æ–¥–Ω—ã—Ö —Å–≤–µ—á–µ–π")
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ —Å–≤–µ—á–∏ –≤ DataFrame
    df = pd.DataFrame(input_candles)
    
    # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≤—Å–µ–π –∏—Å—Ç–æ—Ä–∏–∏
    df = create_advanced_momentum_features(df)
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
    df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É
    last_date = pd.to_datetime(df['date'].iloc[-1])
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–µ 20 —Å–≤–µ—á–µ–π
    predicted_candles = []
    
    for i in range(20):
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        last_row = df.iloc[-1].copy()
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å–≤–µ—á—É
        predictions = predict_single_candle_from_row(last_row)
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å–≤–µ—á—É
        new_candle = {
            'date': (last_date + timedelta(days=i+1)).strftime('%Y-%m-%d'),
            'ticker': last_row.get('ticker', 'UNKNOWN'),
            'open': predictions.get('open', 0),
            'high': predictions.get('high', 0),
            'low': predictions.get('low', 0),
            'close': predictions.get('close', 0),
            'volume': predictions.get('volume', 0)
        }
        
        predicted_candles.append(new_candle)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é —Å–≤–µ—á—É –∫ –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        new_row = last_row.copy()
        new_row.update(predictions)
        new_row['date'] = new_candle['date']
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É –≤ DataFrame
        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
        
        # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏
        df = create_advanced_momentum_features(df)
        df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)
    
    return predicted_candles

def predict_single_candle_from_row(row_data: pd.Series) -> Dict[str, float]:
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ–¥–Ω–æ–π —Å–≤–µ—á–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä–æ–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
        df = pd.DataFrame([row_data])
        
        # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df = create_advanced_momentum_features(df)
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
        missing_features = set(feature_columns) - set(df.columns)
        if missing_features:
            for feature in missing_features:
                df[feature] = 0.0
        
        X = df[feature_columns]
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions = {}
        
        # –û—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –¥–ª—è close
        if model is not None and scaler is not None:
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
            X_scaled = scaler.transform(X)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
            return_prediction = model.predict(X_scaled)[0]
            
            # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            return_prediction = np.clip(return_prediction, -0.1, 0.1)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ü–µ–Ω—É –∑–∞–∫—Ä—ã—Ç–∏—è
            current_close = row_data.get('close', 100.0)
            predicted_close = current_close * (1 + return_prediction)
            predictions['close'] = float(predicted_close)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥—Ä—É–≥–∏—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–π
            recent_ratios = {
                'open': 0.9968,
                'high': 1.0056,
                'low': 0.9906
            }
            
            for col in ['open', 'high', 'low']:
                if col in recent_ratios:
                    noise = np.random.normal(0, 0.001)
                    predictions[col] = float(predicted_close * (recent_ratios[col] + noise))
            
            # Volume –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            current_volume = row_data.get('volume', 1000000)
            volume_noise = np.random.normal(0, 0.1)
            predictions['volume'] = float(current_volume * (1 + volume_noise))
        
        return predictions
        
    except Exception as e:
        import traceback
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
        print(f"Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}")

@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    if not load_models():
        print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –°–æ–∑–¥–∞–π—Ç–µ –º–æ–¥–µ–ª—å —Å–Ω–∞—á–∞–ª–∞.")

@app.get("/")
async def root():
    return {"message": "Optimized Price Prediction API", "status": "running", "version": "2.0.0"}

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "scaler_loaded": scaler is not None,
        "feature_count": len(feature_columns) if feature_columns else 0
    }

@app.post("/upload-data")
async def upload_data(file: UploadFile = File(...)):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    try:
        if not file.filename.endswith(('.csv', '.json')):
            raise HTTPException(status_code=400, detail="–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ CSV –∏–ª–∏ JSON")
        
        file_path = f"data/{file.filename}"
        os.makedirs("data", exist_ok=True)
        
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        if file.filename.endswith('.csv'):
            df = pd.read_csv(file_path)
        else:
            df = pd.read_json(file_path)
        
        return {
            "status": "success",
            "message": f"–§–∞–π–ª {file.filename} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω",
            "filename": file.filename,
            "rows_count": len(df),
            "columns_count": len(df.columns),
            "columns": list(df.columns)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {str(e)}")

@app.post("/train", response_model=TrainingResponse)
async def train_models(
    filename: str = Form(...),
    config: str = Form(...)
):
    """–û–±—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    try:
        file_path = f"data/{filename}"
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail=f"–§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ /upload-data")
        
        try:
            config_dict = json.loads(config)
            training_config = TrainingConfig(**config_dict)
        except json.JSONDecodeError:
            raise HTTPException(status_code=400, detail="–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç JSON –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"–û—à–∏–±–∫–∞ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {str(e)}")
        
        result = train_models_with_config(file_path, training_config)
        
        # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç—å
        load_models()
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {str(e)}")

@app.get("/training-config")
async def get_default_training_config():
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    default_config = TrainingConfig()
    return {
        "default_config": default_config.dict(),
        "description": {
            "train_size": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è",
            "test_size": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (None = –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ)"
        }
    }

@app.get("/data-files")
async def list_data_files():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —Å –¥–∞–Ω–Ω—ã–º–∏"""
    try:
        data_dir = "data"
        if not os.path.exists(data_dir):
            return {"files": [], "message": "–ü–∞–ø–∫–∞ data –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"}
        
        files = []
        for filename in os.listdir(data_dir):
            if filename.endswith(('.csv', '.json')):
                file_path = os.path.join(data_dir, filename)
                file_size = os.path.getsize(file_path)
                files.append({
                    "filename": filename,
                    "size_bytes": file_size,
                    "size_mb": round(file_size / (1024 * 1024), 2)
                })
        
        return {"files": files}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤: {str(e)}")

@app.post("/predict", response_model=PredictionResponse)
async def predict_prices(request: PredictionRequest):
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–∏—Ö 20 —Å–≤–µ—á–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö —Å–≤–µ—á–µ–π"""
    try:
        if model is None:
            raise HTTPException(status_code=503, detail="–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        if not request.params:
            raise HTTPException(status_code=400, detail="–ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(request.params)} –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–≤–µ—á–µ–π")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º 20 —Å–ª–µ–¥—É—é—â–∏—Ö —Å–≤–µ—á–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ–π –∏—Å—Ç–æ—Ä–∏–∏
        predicted_candles = generate_20_candles_from_history(request.params)
        
        print(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(predicted_candles)} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å–≤–µ—á–µ–π")
        return PredictionResponse(predictions=predicted_candles)
        
    except Exception as e:
        import traceback
        print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        print(f"Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8010)
